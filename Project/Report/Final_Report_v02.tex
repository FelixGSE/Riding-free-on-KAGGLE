\documentclass[fleqn]{article}

%opening
\title{15D012 Advanced Computational Methods}
\author{Denitsa Panova, Felix Gutmann, Thomas Vicent√©}
\date{17.03.2016}

\usepackage{graphicx}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{tabularx}
\usepackage{geometry}
\usepackage{setspace}
\usepackage[right]{eurosym}
\usepackage[printonlyused]{acronym}
\usepackage{floatflt}
\usepackage[usenames,dvipsnames]{color}
\usepackage{colortbl}
\usepackage{paralist}
\usepackage{array}
\usepackage{titlesec}
\usepackage{parskip}
\usepackage[right]{eurosym}
    \usepackage[bottom]{footmisc}
%\usepackage{picins}
\usepackage[subfigure,titles]{tocloft}
\usepackage[pdfpagelabels=true]{hyperref}
\lstset{basicstyle=\footnotesize, captionpos=t, breaklines=true, showstringspaces=false, tabsize=2, frame=lines, numbers=left, numberstyle=\tiny, xleftmargin=2em, framexleftmargin=2em}
\usepackage{titlesec}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{pdflscape}
\usepackage{rotating}
\usepackage{lipsum}  
\newcommand{\R}{\textrm{R}}

\begin{document}
\maketitle

\section{Introduction}

\lipsum[2-4]

\section{Review of Models and Predictions}

Table \ref{tab1} gives a broad overview of all the applied methods with the corresponding results on the leaderboard and on cross validation (if available). As depicted we tried and extensive amount of different models. In order to be concise we will briefly line out some basic aproaches and in a little more more detail the approached that worked best for us (both in performance and applyability).

\subsection{Basic "off the shelf" approaches and benchmark}

One of the first models we applied were some penalized (multionomial- ) GLM's using Ridge, Lasso and Elastic Net regularization. This should give as another internal benchmarks for further procedure.  It turned out that all three  didn't perform well isolated ( leaderboard performance $\approx 0.50$).\footnote{Note, that we used it again in one ensemble approach, see later on} (Hmmmmmm)

SVM's are wide used method. The e1071 package provides a flexible setting for SVM's in case of tuning. It provides several kernel types and classification methods (c and nu) and arguments for class imbalances. A grid search was perforemd to tune both C ( cost for missclassifitaion ) and gamma (margin). However, we stopped it after approximatly 10 hours without a result ( grid search with only two parameters for each argument). A submission with radial kernel and basic config obtained a score of 0.506 on the leaderboard.  Mainly due to the lag of speed, but also because of tuning expenses and off the shelf performance SVM wasn't considered anymore for the following process. 

The best "off the shelf" methods turned out to be random forest and boosting. Especially random forest performed extremly well. Due to the speed of the model it is very easy to tune. Table \ref{tab1} showes some of the results for random forest in different settings. At the end it turned out that the overall second best performance was based on a random forest. We found this result quite early in the process. Based on the interim evalutaion results this was adapted benchmark model. 

\subsection{Manuall ensemble approach. }

We briefly describe our so far most promising approach in more detail. Instead of running isolated models, we used different models and average orediction results. 

The first try in that direction uses the superLearner package of R, which allows to run several model types combined. Furthermore, it provides internal cross validation and paralized implementation for some models, which increases performance. Since it doesn't support multiple classification, we transformed the outcome variable to a binary matrix and trained a full model using a mix of gbm, random forest and a lasso glm. Despite of the rather good perfomance on the leaderboard ( $0.543$ ), it couldn't beat the best random forest specifiaction so far. Despite parallelising there is still a speed issue. A full training of the model and prediction lasts approximatly six hours. 

We wanted to exploit this more diverse prediction power in a more flexible setting regarding both model specifiaction and speed performance. We use the H$_2$0 package, which provides an optimized performance of several different models. Furthermore, the supported methods are also parallized, which brings again computational gains. Concerning the data we droped the minority classes 4 and 5 to let the models focus more on the core classes. Furthermore we droped observations, which in out opinion where questionable (as discussed in the introduction). In this setting we applied the following models for out ensembling: 

\begin{itemize}
	\item random forests
	\item gbm
	\item (deep-) neural networks
	\item extremely randomized trees
	\item xgboosts
	\item rule-based models
\end{itemize}

Using such am extensive number of models a grid search of optimal parameters didn't seem to be feasible in reasonable amount of time. Therefore, we re-run all models several times and randomly choose parameters out of a reasonable parameter range \footnote{A "brute force"approach" was considered using an AWS instance. Note that, while using the superLearner we set up and configured an instance on AWS to outsource some computational work and tuning efforts. However, it turned out that the cost benefit ratio here was not appropiate mainly due to stability and monetary issues.}. To monitor our perfomance we randomly subset roughly 66\% of the training data in each iteration as a input data set and use the other part as a test set to validate our models. This procedure turned out to be a solid working basis, because results could be produced fast within approximatly less than an hour. 
Instead of directly predicting classes we used the probabilities for each classes produced by each model. Since we average at the end over all predictions of each model, we believe this provides a more granular outcome. This procedure gave us the best result on the public leaderboard with an accuracy of $0.557$. 

Experience with stacking

\section*{Conclusion and possible improvements}

So far we saw that the best approach so far was the manual ensemble approach. Despite the fact it performed quite good, it also seem to have an "upper bound". Concerning the model setting the only thing would probably be to include SVM's using randomized parameter search.\footnote{In the late process we experimented with some more "unorthodox" approaches. This involved tranforming the data using unsupervised techniques (principle components, clustering etc.) and perform prediction on that data set. The idea was try to condence and rearrange informations in a different setting. However using a random forest gave an (5-fold) cv-average accuracy of only around $0.51$.[OPTIONAL]}  So far in our opinion the only thing that can be done to increase the prediction performance is construct new features, which add more predictive power. 

\pagebreak

\section*{Appendix}

\begin{table}[!h]
	\centering
	\caption{Overview of Methods and results}
	\vspace{0.3cm}
\rotatebox{90}{
\begin{tabular}{|l|l|l|l|l|}
	\hline
	Method & Specification & CV  & LB & Package \\
	\hline
	\textbf{Penalized multonomial GLM} & - & - & &glmnet  \\
	\textbf{... Lasso} & - & - &0.506 &  \\
	\textbf{... Elastic Net} & - & - & &  \\
	\textbf{... Ridge} & - & - & &   \\
	\textbf{Support Vector Machines} & Kernel: Radial &  & $0.496$ & \textrm{e1071}\\
	\textbf{Random Forest}  &    &  &  & \textrm{randomForest}\\
	\textbf{... Off the shelf} 	& - & - &  & randomForest \\
	\textbf{... Balanced DS}  & -  & - & $0.506$ & randomForest, Smooth  \\
	\textbf{Boosting} 			&    &  &  & \textrm{randomForest}\\
	\textbf{Super Learner} & RandomForest, Boosting, &  &$0.543$ & SuperLearner\\
	\textbf{Manual Ensemble} & RandomForest, LassoGLM, &  &$0.543$ & H$_2$0\\
	\textbf{Unsupervised} &  & $0.516$  & & Several\\
\end{tabular}
}
	\vspace{0.3cm}
	\raggedright
	\label{tab1}
\end{table}


\end{document}