---
title: "second part of the report"
author: "Denitsa Panova, Felix Gutmann, Thomas Vicente"
date: "March 16, 2016"
output: html_document
---

# 2 Review of Models and Predictions
Table 1 displays the methods used with the corresponding results on the leader board, and, when available, on cross validation. Rather than describing all the techniques we tried, we will concentrate on the ones which allowed us to achieve a notable prediction power, and ones that seemed relevant for our competition.  

#2.1 Single Method Approach
We first tried multinomial GLMs penalized model with Ridge, Lasso and Elastic Net. They performed rather poorly, achieving around 50% on the leaderboard. But we used them as benchmarks for the future methods. 

We then tried the e1071 package that provided a very tunable implementation of Support Vector Machines,notably we can modify several kernel types, classification methods (c and nu) and arguments for class imbalance. Additionally a grid search was performed to tune both the weights on the misclassification costs and the margin. However, we stopped this procedure after about 10 hours of running. Another submission, using a radial kernel and a basic configuration, obtained a score of 0.506. Mainly due to its expensiveness, SVM wasn’t considered anymore as a relevant alternative.

The best ”off the shelf” methods turned out to be random forest and boosting. Since random forest it computationally fast Notably, they are realtively easy to tune. Table 1 shows some of the results for random forest applying different settings. Based on the private-leader-board results, this is utilizes as the new benchmark model.  

#2.2 Ensemble of Methods Approach
Instead of running isolated models, we use ensemble learning. This machine learning technique consists of letting different 'strong' learners to 'vote' what class each prediction. We expect such generalized method to have smaller variance than the variance of every submodel separately, which increases the predictive power.

The first suchkike approach uses the superLearner package of R. It allows us to combine several model predictions within a meta-learner algorithm. We waht to distinguish how the last step of the algorithm works. In order to understand what is the best class value for each observation and to have some inference, on the last step we use deep learning algorithm and put as inputs the strong-learners predictions from the previous steps and the actual training classes. Furthermore, the package provides an internal cross validation option, as well as parallelized implementation for some models. However, since it doesn’t support multiple classification, we had to transform the outcome variable to a binary matrix and train the model using a mixture of GBM, random forest and Lasso GLM.  This approach yielded an accuracy of 54.3%. This is relatively good, but lower than our best random forest. Also, a full training of the model and prediction lasts approximately six hours.  

We still strongly believed that ensembling could improve our accuracy and tried a more manual method, well tailored for our data. As mentioned before, we represent classes in binary manner. Then we take the mean of the predicted probabilities (to belong to a class) of all the methods; we believe this is a better way than averaging over the raw predictions. The models are run into a loop. In order to uncorrelate the models and introduce randomness, we train them on different varying size random subsamples. Also, at each iteration, the models' parameters are set randomly, within a reasonable range of values, according to multiple tries. 

The chosen models are H2O's implementation of random forest, gradient boosting machine, and deep learning. The last two were chosen because of the parallelized computing option and tuning possibilities. We also included extremely randomized trees, xgboost, and the ensemble rule C5 algorithm. They were run in a loop of up to 40 iterations (so 240 models in total), which takes a reasonable 2 hours running time. It is possible to observe the up-to-date accuracy after each new model.

We also saw our performance improve when we removed classes 4 and 5 from the training sample. At best, this procedure gave us an accuracy of 55.7% on the public leaderboard. A diffrently tuned version, gave up to 54% on cross-validation.

#Conclusion and possible improvements
So far we saw that the best approach was the probability blending ensemble approach. Of course, it is much less optimized than the submodels as it is coded on R. We also would have liked to explore more tuning possibilities. Concerning the model choice, we also could probably have tried an implementation of SVM using randomized parameter search. 

In our opinion, the way to increase the performance is in fact by constructing new features. Out attemps in that regard didn't lead to any improvement.
