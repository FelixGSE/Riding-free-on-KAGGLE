---
title: "second part of the report"
author: "Denitsa Panova, Felix Gutmann, Thomas Vicente"
date: "March 16, 2016"
output: html_document
---
# The things which are in bold are either wrongly spelled or i think that they sound better in the way I have proposed

# 2 Review of Models and Predictions
Table 1 **(in your file it is not directing me to the table)** displays the methods used with the corresponding results on the leader board, and, when available, on cross validation. Rather than describing all the techniques we tried, we will concentrate on the ones which allowed us to achieve a notable prediction power, and others that seemed relevant for our competition.  

#2.1 Single method approach
We first tried multinomial GLMs penalized model with Ridge, Lasso and Elastic Net. The performed rather poorly, achieving around 50% on the leaderboard. But we used them as benchmarks for future methods. 

We then tried the e1071 package that provided a very tunable implementation of Support Vector Machines, notably several kernel types, classification methods (c and nu) and arguments for class imbalance. Additionally a grid search was performed to tune both the weights on the misclassification costs and the margin. However, we stopped this procedure after about 10 hours of run. A submission, using a radial kernel and a basic configuration, obtained a score of 0.506. Mainly due to its expensiveness, SVM wasn’t considered anymore as a relevant alternative.

The best ”off the shelf” methods turned out to be random forest and boosting. Notably, due to random forests' computational intensity, they are realtively easy to tune. Table 1 shows some of the results for random forest with different settings. Based on the result on the private leader board, this became the new benchmark model.  

#2.2 Ensemble of methods approach
Instead of running isolated models, we used ensemble learning. This consists in letting different more or less 'strong' learners to vote for each prediction. We expect such "generalizer" method to have a smaller variance than its submodels separately, increasing the predictive power.

The first version in that direction uses the superLearner package of R, which allows to combine several models' predictions within a meta-learner algorithm. The last inference's step is done on the predictions, rather than on the original variables. Furthermore, the package provides an internal cross validation option, as well as a parallelized implementation for some models. But since it doesn’t support multiple classification, we had to transform the outcome variable to a binary matrix and trained the model using a mixture of GBM, random forest and Lasso GLM.  This approach yielded an accuracy to 54.3%. This is relatively good, but lower than our best random forest. Also, a full training of the model and prediction lasts approximately six hours.  

We still strongly believed that ensembling could improve our accuracy and tried a more manual method, well tailored for our data. It consists in taking the mean of the predicted probabilities to belong to a class over different method; we believe this provides a more granular results. The models are run into a loop. In order to uncorrelate the models, we train them on different varying size random subsamples. Also, at each iteration, the models' parameters are set randomly, within a reasonable range of values, according to multiple tries. 

The chosen models are H2O's implementation of random forest, gradient boosting machines, and deep learning. The latter were chosen because of the parallelized computing option and tuning possibilities. We also included extremely randomized trees, xgboost, and the ensemble rules C5 algorithm. They were run in a loop of up to 40 iterations (so 240 models in total), which takes a reasonable 2 hours running time. It is possible to observe the up-to-date accuracy after each new model.

We also saw our performance improve when removing classes 4 and 5 from the training sample. At best, this procedure gave us the an accuracy of 55.7% on the public leaderboard. A diffrently tuned version, gave up to 54% on cross-validation.

#Conclusion and possible improvements
So far we saw that the best approach was the probability blending ensemble approach. Of course, it is much less optimized than the submodels as it is coded on R. We also would have liked to explore more tuning possibilities. Concerning the model choice, we also could probably have tried an implementation of SVM using randomized parameter search. 

In our opinion, the way to increase the performance is in fact by constructing new features. Out attemps in that regard didn't lead to an improvement.
