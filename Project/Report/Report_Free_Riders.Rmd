---
title: "Report_Free_Riders"
author: "Denitsa Panova, Felix Gutmann, Thomas Vicente"
date: "March 17, 2016"
output: html_document
---

# Introducion

This report describes the results and approaches undertaken by the Free Riders team for a Kaggle competion done in the context of the Advanced Computational Methods and Machine Learning classes at the Barcelona Graduate School of Economics, Spain.  

This competition is about predicting the popularity of Mashable.com's news articles. The original target variable, the number of shares, has been transformed into a categorical variable, ranging from 1 (least popular) to 5 (viral). The whole data set is divided into two parts - training (30 000 obsrevations) and testing (9 644 observations). In order to predict popularity classes we utilize will use computational statistics and machine learning. In particular, we implement various algorithms, ranging from a single model type (e.g. Random forest) to multiple combinations of them.  A more detailed discussion regarding these methods can be found in the third part of the report. Before, that we shall discuss the way we explored the data.  

# Data Exploration

The data set, acquired and preprocessed by K. Fernandes et al. has 59 features. In order to acquire more extensive knowledge of the provided data set, we conduct data exploration. This consists of summarizing the distribution of the data, and its conditional distribution regarding the categorical target [variable](https://github.com/FelixGSE/Riding-free-on-KAGGLE/blob/master/Project/Work/R_Scripts/F00_DataInspection.R).  

We also tried to identify "ambiguous" data points that would alter our inference negatively. For example, observation 16546,has a "kw_max_avg" value that is considered senseless regarding the rest of the distribution. In fact, many observations were easily detected because they had the same extreme value for some variables. We [store](https://github.com/FelixGSE/Riding-free-on-KAGGLE/blob/master/Project/Data/Raw_Data/outliers_id) the ID of such points, into a data set in order to later exclude the problematic observations.  We evaluated our performance when using the full data, and the reduced data. This led us to prefer the second option for most runs.  

Also we have noticed that the data set is imbalanced. Initially we used Synthetic Minority Over-sampling Technique to balance the data set by augmenting classes 4 and 5. However, this approach did not increase our prediction power. Therefore, we focused on classifiers which can deal with imbalanced data like Random Forest.  

At the end of our data explortion, we thought about creating new features from the original ones. We notably scraped google news popularity results based on the URL feature. From it we took the article's title and the date. This approach unexpectedly only inconsistently improved the Random Forest accuracy.  

# Review of Models and Predictions
Table 1 displays the methods used with the corresponding results on the leader board, and, when available, on cross validation. Rather than describing all the techniques we tried, we will concentrate on the ones which allowed us to achieve a notable prediction power, and ones that seemed relevant for our competition.  

## Single Method Approach
We first tried multinomial [GLMs penalized](https://github.com/FelixGSE/Riding-free-on-KAGGLE/blob/master/Project/Work/R_Scripts/F11_PenalizedGLM_v01.R) model with Ridge, Lasso and Elastic Net. They performed rather poorly, achieving around 50% on the leaderboard. But we used them as benchmarks for the future methods. 

We then tried the [e1071](https://github.com/FelixGSE/Riding-free-on-KAGGLE/blob/master/Project/Work/R_Scripts/F12_SVM.R) package that provided a very tunable implementation of Support Vector Machines,notably we can modify several kernel types, classification methods (c and nu) and arguments for class imbalance. Additionally a grid search was performed to tune both the weights on the misclassification costs and the margin. However, we stopped this procedure after about 10 hours of running. Another submission, using a radial kernel and a basic configuration, obtained a score of 0.506. Mainly due to its expensiveness,SVM  wasn’t considered anymore as a relevant alternative.

The best ”off the shelf” methods turned out to be random forest and boosting. Since random forest it computationally fast Notably, they are realtively easy to tune. Table 1 shows some of the results for random forest applying different settings. Based on the private-leader-board results, this is utilizes as the new benchmark model.  

## Ensemble of Methods Approach
Instead of running isolated models, we use ensemble learning. This machine learning technique consists of letting different 'strong' learners to 'vote' what class each prediction. We expect such generalized method to have smaller variance than the variance of every submodel separately, which increases the predictive power.

The first suchkike approach uses the [superLearner](https://github.com/FelixGSE/Riding-free-on-KAGGLE/blob/master/Project/Work/R_Scripts/F15_SuperLearner.r) package of R. It allows us to combine several model predictions within a meta-learner algorithm. We waht to distinguish how the last step of the algorithm works. In order to understand what is the best class value for each observation and to have some inference, on the last step we use deep learning algorithm and put as inputs the strong-learners predictions from the previous steps and the actual training classes. Furthermore, the package provides an internal cross validation option, as well as parallelized implementation for some models. However, since it doesn’t support multiple classification, we had to transform the outcome variable to a binary matrix and train the model using a mixture of GBM, random forest and Lasso GLM.  This approach yielded an accuracy of 54.3%. This is relatively good, but lower than our best random forest. Also, a full training of the model and prediction lasts approximately six hours.  

We still strongly believed that ensembling could improve our accuracy and tried a more manual method, well tailored for our data. As mentioned before, we represent classes in binary manner. Then we take the mean of the predicted probabilities (to belong to a partticular class) of all the methods in the ensembling method; we believe this is a better way than averaging over the raw predictions. The models are run into a loop. In order to uncorrelate the models and introduce randomness, we train them on different varying size random subsamples. Also, at each iteration, the models' parameters are set randomly, within a reasonable range of values, according to multiple tries. 

The chosen models are H2O's implementation of [random forest, gradient boosting machine, and deep learning.](https://github.com/FelixGSE/Riding-free-on-KAGGLE/blob/master/Project/Work/R_Scripts/TriesFinalCV.R) The last two were chosen because of the parallelized computing option and tuning possibilities. We also included extremely randomized trees, xgboost, and the ensemble rule C5 algorithm. They were run in a loop of up to 40 iterations (so 240 models in total), which takes a reasonable 2 hours running time. It is possible to observe the up-to-date accuracy after each new model.

We also saw our performance improve when we removed classes 4 and 5 from the training sample. At best, this procedure gave us an accuracy of 55.7% on the public leaderboard. A diffrently tuned version, gave up to 54% on cross-validation.

# Conclusion and possible improvements
So far we saw that the best approach was the probability blending ensemble approach. Of course, it is much less optimized than the submodels as it is coded on R. We also would have liked to explore more tuning possibilities. Concerning the model choice, we also could probably have tried an implementation of SVM using randomized parameter search. 

In our opinion, the way to increase the performance is in fact by constructing new features. Out attemps in that regard didn't lead to any improvement.
